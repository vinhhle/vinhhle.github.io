[
  {
    "path": "posts/income prediction/",
    "title": "Predict income with logistic regression an two-class classification",
    "description": {},
    "author": [
      {
        "name": "Vinh Hung le",
        "url": "vinhhle.github.io"
      }
    ],
    "date": "2021-08-14",
    "categories": [],
    "contents": "\n\nContents\nData Cleaning\nRe-grouping\n\nData Exploratory Analysis\nAge\nWork class\nEducation\nMarital status\nOccupation\nRace\n\nModelling\nOverview of logistic regression\nSimple logistic regression\nMultiple logistic regression\nKey metrics\nROC and AUC\n\n\nData Cleaning\nIn this project, we’ll work with the 1994 Census database done by Barry Becker. Our task is to predict whether the annual income of an individual is high (greater than $50,000) using provided features.\n\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(plyr)\nlibrary(ROCR)\nlibrary(plotly)\nfont = \"Avenir Next\"\ntext_color = \"#353D42\"\nadult <- read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', \n                    sep = ',', \n                    fill = F, \n                    strip.white = T) %>% \n  drop_na()\ncolnames(adult) <- c('age', 'work', 'fnlwgt', 'edu', 'edu_num', 'marital', 'job', 'relationship', \n                     'race', 'sex', 'capital_gain', 'capital_loss', 'hours', 'nation', 'income')\nrmarkdown::paged_table(adult)\n\n\n\n\nThere are 32,561 observations with 15 variables in the dataset. Most of the variables are self-explanatory themselves.\n## Removing variables\nTo simplify the analysis, we may drop some unnecessary variables, including fnlwgt (standing for final weight), edu (represented by edu_num), and relationship (represented by marital and sex).\n\n\nadult <- adult %>%\n  select(!c(fnlwgt, edu, relationship))\n\n\nThe variable nation represents the native country of the individuals. Let’s see where they come from.\n\n\nadult %>% \n  group_by(nation) %>% \n  dplyr::summarize(\n    count = n()\n  ) %>% \n  arrange(desc(count))\n\n# A tibble: 42 × 2\n   nation        count\n   <chr>         <int>\n 1 United-States 29170\n 2 Mexico          643\n 3 ?               583\n 4 Philippines     198\n 5 Germany         137\n 6 Canada          121\n 7 Puerto-Rico     114\n 8 El-Salvador     106\n 9 India           100\n10 Cuba             95\n# … with 32 more rows\n\nNearly 90% of individuals in the dataset come from the US. Thus, we may exclude observations from other countries and then drop the nation variable for simpler analysis.\n\n\nadult_us <- adult %>% \n  filter(nation == \"United-States\") %>% \n  select(!nation)\n\n\nThe two continuous variables capital_gain and capital_loss represent the money each individual gained or lost from their financial investments.\n\n\nloss <- ggplot(adult_us, aes(x = capital_loss, group = income, fill = income)) +\n  geom_histogram(bins = 10, colour = \"white\") +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0), name = \"loss (USD)\")+\n  scale_fill_manual(values = c(\"#0072B2\", \"#D55E00\")) +\n  theme(\n    panel.background = element_blank(),\n    axis.line.x = element_line(size = 0.2, color = text_color),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 11),\n    axis.title = element_text(family = font, color = text_color, size = 11), \n    legend.position = \"none\"\n  )\ngain <- ggplot(adult_us, aes(x = capital_gain, group = income, fill = income)) +\n  geom_histogram(bins = 10, colour = \"white\") +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0), name = \"gain (USD)\")+\n  scale_fill_manual(values = c(\"#0072B2\", \"#D55E00\")) +\n  theme(\n    panel.background = element_blank(),\n    axis.line.x = element_line(size = 0.2, color = text_color),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 11),\n    axis.text.y = element_blank(),\n    axis.title = element_text(family = font, color = text_color, size = 11),\n    legend.position = c(0.5, 0.5),\n    legend.text = element_text(family = font, color = text_color, size = 11),\n    legend.title = element_text(family = font, color = text_color, size = 11)\n  )\nplot_grid(\n  loss, NULL, gain,\n  nrow = 1, align = 'hv', rel_widths = c(1, .04, 1, .04, 1))\n\n\n\nBoth graphs are highly skewed to the left, meaning that almost all people have zero capital gain or loss. Therefore, we can also drop both variables from the dataset for simpler analysis.\n\n\nadult_us <- adult_us %>% \n  select(!c(capital_gain, capital_loss))\n\n\nRe-grouping\nThe variable work represents the industry in which individuals are working. We can group 9 levels into 4 levels: government, private, self_employed, and others.\nThe variable marital represents the marital status of individuals. We can group 7 levels into 5 levels: single, married, separate, divorced, and widowed.\nThe variable job represents the occupation of individuals. We can group 15 levels into 6 levels: white collar, blue collar, service, professional, sales, and and widowed.\n\n\nadult_us <- adult_us %>%\n  mutate(\n    work = case_when(\n      work == \"?\" ~ \"others\",\n      work == \"Never_worked\" ~ \"others\",\n      work == \"Without-pay\" ~ \"others\",\n      work == \"Federal-gov\" ~ \"gov\",\n      work == \"Local-gov\" ~ \"gov\",\n      work == \"State-gov\" ~ \"gov\",\n      work == \"Self-emp-inc\" ~ \"self-emp\",\n      work == \"Self-emp-not-inc\" ~ \"self-emp\",\n      work == \"Private\" ~ \"private\",\n      TRUE ~ \"others\"\n    ),\n    marital = case_when(\n      marital == \"Divorced\" ~ \"divorced\",\n      marital == \"Married-AF-spouse\" ~ \"married\",\n      marital == \"Married-civ-spouse\" ~ \"married\",\n      marital == \"Married-spouse-absent\" ~ \"married\",\n      marital == \"Never-married\" ~ \"single\",\n      marital == \"Separated\" ~ \"separate\",\n      marital == \"Widowed\" ~ \"widowed\"\n    ),\n    job = case_when(\n      job == \"?\" ~ \"others\",\n      job == \"Adm-clerical\" ~ \"white-collar\",\n      job == \"Armed-Forces\" ~ \"others\",\n      job == \"Craft-repair\" ~ \"blue-collar\",\n      job == \"Exec-managerial\" ~ \"white-collar\",\n      job == \"Farming-fishing\" ~ \"blue-collar\",\n      job == \"Handlers-cleaners\" ~ \"blue-collar\",\n      job == \"Machine-op-inspct\" ~ \"blue-collar\",\n      job == \"Other-service\" ~ \"service\",\n      job == \"Priv-house-serv\" ~ \"service\",\n      job == \"Prof-specialty\" ~ \"professional\",\n      job == \"Protective-serv\" ~ \"service\",\n      job == \"Sales\" ~ \"sales\",\n      job == \"Tech-support\" ~ \"service\",\n      job == \"Transport-moving\" ~ \"blue-collar\"\n    )\n  ) \n\n\nThe dataset now contains 29,170 observations with 9 variables.\n\n\nsummary(adult_us)\n\n      age            work              edu_num     \n Min.   :17.00   Length:29170       Min.   : 1.00  \n 1st Qu.:28.00   Class :character   1st Qu.: 9.00  \n Median :37.00   Mode  :character   Median :10.00  \n Mean   :38.66                      Mean   :10.17  \n 3rd Qu.:48.00                      3rd Qu.:12.00  \n Max.   :90.00                      Max.   :16.00  \n   marital              job                race          \n Length:29170       Length:29170       Length:29170      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n     sex                hours          income         \n Length:29170       Min.   : 1.00   Length:29170      \n Class :character   1st Qu.:40.00   Class :character  \n Mode  :character   Median :40.00   Mode  :character  \n                    Mean   :40.45                     \n                    3rd Qu.:45.00                     \n                    Max.   :99.00                     \n\nData Exploratory Analysis\nAge\n\n\nggplot(adult_us, aes(x = age, group = income, fill = income)) +\n  geom_histogram(binwidth = 2, colour = \"white\") +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0), name = \"age\")+\n  scale_fill_manual(values = c(\"#0072B2\", \"#D55E00\")) +\n  theme(\n    panel.background = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 11),\n    axis.title = element_text(family = font, color = text_color, size = 11),\n    legend.position = c(0.8, 0.6),\n    legend.text = element_text(family = font, color = text_color, size = 11),\n    legend.title = element_text(family = font, color = text_color, size = 11)\n  )\n\n\n\nThe graph shows that most individuals earn fewer than $50,000 per year. In the low-income group, incomes tend to increase with age. In the high-income group, most are in their mid-career, ranging between 30 and 50 years old.\nWork class\n\n\nadult_work <- adult_us %>% \n  group_by(work, income) %>% \n  tally() %>% \n  arrange(work, income)%>% \n  mutate(income = factor(income, levels = c(\">50K\", \"<=50K\"))) %>% \n  group_by(work) %>% \n  dplyr::mutate(percentage = round(n/sum(n)*100, 1)) \nwork <- ggplot(adult_work, aes(x = work, y = percentage, fill = income)) +\n  geom_col(position = \"stack\", color = \"white\", size = 0.3, width = 1) +\n  scale_x_discrete(expand = c(0,0), name = NULL) +\n  scale_y_continuous(expand = c(0,0), name = \"percentage\") +\n  coord_cartesian(clip = \"off\") +\n  scale_fill_manual(values = c(\"#D55E00\", \"#0072B2\"),\n                    breaks = c(\">50K\", \"<=50K\"),\n                    labels = c(\"high income\", \"low income\"),\n                    name = NULL) +\n  theme(panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 11),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.text = element_text(family = font, color = text_color, size = 11),\n        legend.position = \"bottom\",\n        legend.justification = \"center\",\n        legend.spacing.x = grid::unit(7, \"pt\"),\n        legend.spacing.y = grid::unit(0, \"cm\")\n    )\nggplotly(work, dynamicTicks = TRUE, tooltip = \"y\") %>% \n  layout(hovermode = \"x\") \n\n\n\nThe chart shows that selt-employed individuals tend to have higher incomes, followed by government and private.\nEducation\n\n\nadult_edu <- adult_us %>% \n  group_by(edu_num, income) %>% \n  tally() %>% \n  arrange(edu_num, income)%>% \n  mutate(income = factor(income, levels = c(\">50K\", \"<=50K\"))) %>% \n  group_by(edu_num) %>% \n  dplyr::mutate(percentage = round(n/sum(n)*100, 1)) \nedu <- ggplot(adult_edu, aes(x = edu_num, y = percentage, fill = income)) +\n  geom_col(position = \"stack\", color = \"white\", size = 0.3, width = 1) +\n  scale_x_continuous(expand = c(0,0), name = NULL, seq(0,16,1)) +\n  scale_y_continuous(expand = c(0,0), name = \"count\") +\n  coord_cartesian(clip = \"off\") +\n  scale_fill_manual(values = c(\"#D55E00\", \"#0072B2\"),\n                    breaks = c(\">50K\", \"<=50K\"),\n                    labels = c(\"high income\", \"low income\"),\n                    name = NULL) +\n  theme(panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 11),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.text = element_text(family = font, color = text_color, size = 11),\n        legend.position = \"bottom\",\n        legend.justification = \"center\",\n        legend.spacing.x = grid::unit(7, \"pt\"),\n        legend.spacing.y = grid::unit(0, \"cm\")\n    )\nggplotly(edu, tooltip = \"y\") %>% \n  layout(hovermode = \"x\") \n\n\n\nIt is very obvious that the chance of earning more than $50,000 increases with levels of education.\nMarital status\n\n\nadult_marital <- adult_us %>% \n  group_by(marital, income) %>% \n  tally() %>% \n  arrange(marital, income)%>% \n  mutate(income = factor(income, levels = c(\">50K\", \"<=50K\"))) %>% \n  group_by(marital) %>% \n  mutate(percentage = round(n/sum(n)*100, 1)) \nedu <- ggplot(adult_marital, aes(x = marital, y = percentage, fill = income)) +\n  geom_col(position = \"stack\", color = \"white\", size = 0.3, width = 1) +\n  scale_x_discrete(expand = c(0,0), name = NULL) +\n  scale_y_continuous(expand = c(0,0), name = \"count\") +\n  coord_cartesian(clip = \"off\") +\n  scale_fill_manual(values = c(\"#D55E00\", \"#0072B2\"),\n                    breaks = c(\">50K\", \"<=50K\"),\n                    labels = c(\"high income\", \"low income\"),\n                    name = NULL) +\n  theme(panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 11),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.text = element_text(family = font, color = text_color, size = 11),\n        legend.position = \"bottom\",\n        legend.justification = \"center\",\n        legend.spacing.x = grid::unit(7, \"pt\"),\n        legend.spacing.y = grid::unit(0, \"cm\")\n    )\nggplotly(edu, tooltip = \"y\") %>% \n  layout(hovermode = \"x\") \n\n\n\nNearly a half of married individuals earn high income, while single and separate people are not very likely to earn more than $50,000 a year.\nOccupation\n\n\nadult_job <- adult_us %>% \n  group_by(job, income) %>% \n  tally() %>% \n  arrange(job, income)%>% \n  mutate(income = factor(income, levels = c(\">50K\", \"<=50K\"))) %>% \n  group_by(job) %>% \n  mutate(percentage = round(n/sum(n)*100, 1)) \njob <- ggplot(adult_job, aes(x = job, y = percentage, fill = income)) +\n  geom_col(position = \"stack\", color = \"white\", size = 0.3, width = 1) +\n  scale_x_discrete(expand = c(0,0), name = NULL) +\n  scale_y_continuous(expand = c(0,0), name = \"count\") +\n  coord_cartesian(clip = \"off\") +\n  scale_fill_manual(values = c(\"#D55E00\", \"#0072B2\"),\n                    breaks = c(\">50K\", \"<=50K\"),\n                    labels = c(\"high income\", \"low income\"),\n                    name = NULL) +\n  theme(panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 11),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.text = element_text(family = font, color = text_color, size = 11),\n        legend.position = \"bottom\",\n        legend.justification = \"center\",\n        legend.spacing.x = grid::unit(7, \"pt\"),\n        legend.spacing.y = grid::unit(0, \"cm\")\n    )\nggplotly(job, tooltip = \"y\") %>% \n  layout(hovermode = \"x\") \n\n\n\nPeople with professional occupations have the highest chance earning high income, followed by white collar and sales. Blue collar and service individuals have lower chances.\nRace\n\n\nadult_race <- adult_us %>% \n  group_by(race, income) %>% \n  tally() %>% \n  arrange(race, income)%>% \n  mutate(income = factor(income, levels = c(\">50K\", \"<=50K\"))) %>% \n  group_by(race) %>% \n  mutate(percentage = round(n/sum(n)*100, 1)) \nrace <- ggplot(adult_race, aes(x = race, y = percentage, fill = income)) +\n  geom_col(position = \"stack\", color = \"white\", size = 0.3, width = 1) +\n  scale_x_discrete(expand = c(0,0), name = NULL) +\n  scale_y_continuous(expand = c(0,0), name = \"count\") +\n  coord_cartesian(clip = \"off\") +\n  scale_fill_manual(values = c(\"#D55E00\", \"#0072B2\"),\n                    breaks = c(\">50K\", \"<=50K\"),\n                    labels = c(\"high income\", \"low income\"),\n                    name = NULL) +\n  theme(panel.background = element_blank(),\n        panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 8),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.text = element_text(family = font, color = text_color, size = 11),\n        legend.position = \"bottom\",\n        legend.justification = \"center\",\n        legend.spacing.x = grid::unit(7, \"pt\"),\n        legend.spacing.y = grid::unit(0, \"cm\")\n    )\nggplotly(race, tooltip = \"y\") %>% \n  layout(hovermode = \"x\") \n\n\n\nThe graph shows that white and Asian-Pacific-Islander individuals have higher chance of earning high income than people from other race groups.\nModelling\nBefore modelling the dataset, two things need to be done:\nConvert the variable income into numeric values: 0 for low income and 1 for high income\nDivide the dataset into two parts: 80% for training and 20% for testing\n\n\nadult_us <- adult_us %>% \n  mutate(\n    income = case_when(\n      income == \"<=50K\" ~ 0,\n      income == \">50K\" ~ 1\n    )\n  )\nset.seed(1)\ntrain <- sample(nrow(adult_us), nrow(adult_us)*0.7)\nadult_train <- adult_us[train,]\nadult_test <- adult_us[-train,]\n\n\nOverview of logistic regression\nIn logistic regression, we measure the probability of the dependent variable income based on other variables. For example, given the age, marital status, working hour, race, .etc the model can predict the probability of an individual having a high income.\nThe losgistic function is as following:\n\\(p(income) = \\frac{e^{β_{0} + β_{1}age + β_{2}work + β_{3}edu.num+ β_{4}marital+ β_{5}job+ β_{6}race+ β_{7}sex+ β_{8}hours}}{1 + e^{β_{0} + β_{1}age + β_{2}work + β_{3}edu.num+ β_{4}marital+ β_{5}job+ β_{6}race+ β_{7}sex+ β_{8}hours}}\\)\nThe output of this function ranges between 0 and 1. We can estimate the coefficients by using maximum likelihood.\nSimple logistic regression\nLet’s start with predict income based on the number of education years. The above model is simplified as:\n\\(p(income|edunum) = \\frac{e^{β_{0} + β_{1}edunum}}{1 + e^{β_{0} + β_{1}edunum}}\\)\n\n\nlogis.edu <- glm(income ~ edu_num, data = adult_train, family = binomial)\nsummary(logis.edu)\n\n\nCall:\nglm(formula = income ~ edu_num, family = binomial, data = adult_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5670  -0.6736  -0.5680  -0.1913   2.9593  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.115615   0.090961  -56.24   <2e-16 ***\nedu_num      0.374788   0.008094   46.31   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 22752  on 20418  degrees of freedom\nResidual deviance: 20242  on 20417  degrees of freedom\nAIC: 20246\n\nNumber of Fisher Scoring iterations: 4\n\nBased on the results, the model becomes:\n\\(p(income|edunum) = \\frac{e^{-5.12 + 0.37*edunum}}{1 + e^{-5.12 + 0.37*edunum}}\\)\nThe coefficient \\(β_{1}\\) = 0.37 > 0 implies that people with more years of education are more likely to earn higher income. This confirms what we found out in the exploratory analysis.\nThe model can also help us compare the probability of having high income in two individuals. Take an example with 10 and 15 years of education:\n\\(p(income|edunum = 5) = \\frac{e^{-5.12 + 0.37*10}}{1 + e^{-5.12 + 0.37*10}} = 19.5%\\)\n\\(p(income|edunum = 10) = \\frac{e^{-5.12 + 0.37*15}}{1 + e^{-5.12 + 0.37*15}} = 60.6%\\)\nPeople with 10 years of education only have 19.5% of earning a high income, while the number for those with 15 years of education is up to 60.6%. The difference is very clear.\nHow about income by gender?\n\n\nlogis.sex <- glm(income ~ sex, data = adult_train, family = binomial)\nlogis.sex\n\n\nCall:  glm(formula = income ~ sex, family = binomial, data = adult_train)\n\nCoefficients:\n(Intercept)      sexMale  \n     -2.093        1.305  \n\nDegrees of Freedom: 20418 Total (i.e. Null);  20417 Residual\nNull Deviance:      22750 \nResidual Deviance: 21640    AIC: 21640\n\nBased on the results, the model becomes:\n\\(p(income|sex) = \\frac{e^{-2.093 + 1.305*sex}}{1 + e^{-2.093 + 1.305*sex}}\\)\nThe coefficient \\(β_{1}\\) = 1.305 > 0 implies that men tend to have higher income than women. More specifically:\n\\(p(income|sex = 1/men) = \\frac{e^{-2.093 + 1.305*1}}{1 + e^{-2.093 + 1.305*1}} = 31.3%\\)\n\\(p(income|sex = 0/women) = \\frac{e^{-2.093 + 1.305*0}}{1 + e^{-2.093 + 1.305*0}} = 11%\\)\nMultiple logistic regression\nNow we predict income using all variables in the dataset.\n\n\nlogis_all <- glm(income ~ ., data = adult_train, family = binomial(\"logit\"))\nsummary(logis_all)\n\n\nCall:\nglm(formula = income ~ ., family = binomial(\"logit\"), data = adult_train)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.67929  -0.59669  -0.25559  -0.06119   3.09286  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -9.443193   0.300293 -31.447  < 2e-16 ***\nage                     0.028945   0.001849  15.655  < 2e-16 ***\nworkothers             -1.502802   0.815022  -1.844 0.065201 .  \nworkprivate             0.152157   0.060588   2.511 0.012028 *  \nworkself-emp           -0.080547   0.078106  -1.031 0.302421    \nedu_num                 0.331824   0.010972  30.242  < 2e-16 ***\nmaritalmarried          1.991771   0.073996  26.917  < 2e-16 ***\nmaritalseparate        -0.023178   0.176495  -0.131 0.895521    \nmaritalsingle          -0.556187   0.091718  -6.064 1.33e-09 ***\nmaritalwidowed         -0.033796   0.167532  -0.202 0.840130    \njobothers               1.189078   0.814245   1.460 0.144196    \njobprofessional         0.867857   0.076681  11.318  < 2e-16 ***\njobsales                0.539659   0.070532   7.651 1.99e-14 ***\njobservice              0.281145   0.076389   3.680 0.000233 ***\njobwhite-collar         0.865899   0.059108  14.649  < 2e-16 ***\nraceAsian-Pac-Islander  0.825455   0.311305   2.652 0.008011 ** \nraceBlack               0.135415   0.249836   0.542 0.587807    \nraceOther              -0.239212   0.498374  -0.480 0.631239    \nraceWhite               0.322017   0.237493   1.356 0.175131    \nsexMale                 0.413756   0.058918   7.023 2.18e-12 ***\nhours                   0.030634   0.001823  16.802  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 22752  on 20418  degrees of freedom\nResidual deviance: 15062  on 20398  degrees of freedom\nAIC: 15104\n\nNumber of Fisher Scoring iterations: 6\n\nWe can use the model to predict income in the test dataset. To measure the accuracy of the model, we take a look at the confusion matrix, which summarizes the decision possibilities.\n\n\nconfusion <- data.frame(predicted_no = c(\"True Negative\", \"False Positive\",\"Actual No\"),\n           predicted_yes = c(\"False Negative\", \"True Positive\",\"Actual Yes\"),\n           \"3\" = c(\"Predicted No\", \"Predicted Yes\",\"Total\"), \n           row.names = c(\"actual_no\", \"actual_yes\", \"\")\n           )\ncolnames(confusion) = c(\"predicted_no\", \"predicted_no\", \"\")\nconfusion\n\n             predicted_no   predicted_no              \nactual_no   True Negative False Negative  Predicted No\nactual_yes False Positive  True Positive Predicted Yes\n                Actual No     Actual Yes         Total\n\n\n\npredicted <- predict(logis_all, newdata = adult_test, type = \"response\")\nglm.pred = rep(1, nrow(adult_test))\nglm.pred[as.numeric(predicted) <= 0.5] = 0\nconfusion_matrix <- table(glm.pred, adult_test$income)\ncolnames(confusion_matrix) <- c(\"predicted:0\", \"predicted:1\")\nrownames(confusion_matrix) <- c(\"actual:0\", \"actual:1\")\nconfusion_matrix\n\n          \nglm.pred   predicted:0 predicted:1\n  actual:0        6086        1048\n  actual:1         503        1114\n\nKey metrics\nSome important things that we need to know about the above logistic regression model:\nAccuracy: how often is the classifier correct?\n\n\naccuracy = (confusion_matrix[1] + confusion_matrix[4])/sum(confusion_matrix)\naccuracy\n\n[1] 0.8227631\n\nThe accuracy of the model is up to 82.3%, which is very high. However, it can be sometimes misleading, especially when the prevalence is extremely high or low.\nError rate: how often is the classifier wrong?\n\n\nerror = (confusion_matrix[2] + confusion_matrix[3])/sum(confusion_matrix)\nerror\n\n[1] 0.1772369\n\nPrevalence: how often does the yes occur in the sample\n\n\nprevalence = (confusion_matrix[2] + confusion_matrix[4])/sum(confusion_matrix)\nprevalence\n\n[1] 0.1847789\n\nPrecision: how often is the classifier correct when prediction is yes\n\n\nprevalance = (confusion_matrix[2] + confusion_matrix[4])/sum(confusion_matrix)\nprevalance\n\n[1] 0.1847789\n\nROC and AUC\nROC stands for Receiver Operating Curve. This curve is created by plotting true positive rate (sensitivity) against false positive rate. Below is the ROC of the above model.\n\n\npr <- prediction(predicted, adult_test$income)\nprf <- performance(pr, measure = \"tpr\", x.measure = \"fpr\")\ndd <- data.frame(FP = prf@x.values[[1]], TP = prf@y.values[[1]])\nggplot() +\n  geom_line(data = dd, aes(x = FP, y = TP, color = 'Logistic Regression')) +\n  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1)) +\n  scale_x_continuous(name = \"false positive\") +\n  scale_y_continuous(name = \"true positive\")+\n  theme(panel.background = element_blank(),\n        panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.5),\n        axis.text = element_text(family = font, color = text_color, size = 11),\n        axis.title = element_text(family = font, color = text_color, size = 11),\n        axis.text.x = element_text(margin = margin(t = 15)),\n        axis.ticks = element_blank(),\n        legend.position = \"none\"\n  )\n\n\n\nA ROC curve illustrates the price we have to pay in terms of false positive rate to increase the true positive rate. The further the ROC is away from the diagonal, the better. A perfect ROC curve passes through (0,1), meaning it can classify all positives correctly without any false positive.\nFrom the chart above, we can see the ROC behaves quite well, which suggests a good model for this dataset.\nAUC stands for “Area Under the Curve”, namely the area between the ROC. This value ranges from 0 to 1. However, it often lies between 0.5 and 1. AUC = 1 indicates a perfect classifier, while AUC = 0.5 indicates a random classifier.\n\n\nperformance(pr, measure = \"auc\")@y.values[[1]]\n\n[1] 0.8755168\n\nIn this case, the model’s AUC is 0.875.\n\n\n\n",
    "preview": "https://images.pexels.com/photos/4386324/pexels-photo-4386324.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2",
    "last_modified": "2022-12-13T20:56:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/wine pattern PCA/",
    "title": "Principle component analysis: What make wines different from each other?",
    "description": {},
    "author": [
      {
        "name": "Vinh Hung le",
        "url": "vinhhle.github.io"
      }
    ],
    "date": "2021-03-05",
    "categories": [],
    "contents": "\n\nContents\nThe Dataset\nWhat is PCA?\nHow PCA Works\nInterpret PCA Results\n\n\n\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(ggbiplot)\nlibrary(reshape2) #melt()\nlibrary(colorspace)\nlibrary(cowplot)\nlibrary(Stat2Data)\nlibrary(RColorBrewer)\nlibrary(plotly)\n\n\nThe Dataset\nIn this project, we work with the Wine Data Set from UC Irvine Machine Learning Repository.\nThe dataset contains 178 observations, which result from a chemical analysis of wines derived from 3 different types.\nThere are 13 features that represent 13 components in the wines. These include:\n- Alcohol\n- Malic Acid\n- Ash\n- Alcalinity of Ash\n- Magnesium\n- Total Phenols\n- Flavanoids\n- Nonflavanoid Phenols\n- Proanthocyanins\n- Color Intensity\n- Hue\n- OD280/D315 of Diluted Wines\n- Proline\n\n\ndata(wine)\nwine$Class = wine.class\ncolnames(wine) = c(\"Alcohol\", \"Malic\", \"Ash\", \"AlcAsh\", \"Mg\", \"Phenol\", \"Flav\", \n                   \"NonFlav\", \"Proa\", \"Color\", \"Hue\", \"OD\", \"Proline\", \"Class\")\nrmarkdown::paged_table(head(wine))\n\n\n\n\nSome questions:\nWhich component is more dominant in each class of wine?\nWhat is the relationship between each component? Ex: Do wines with a high level of malic acid also have a high level of manganese or a low level of flavonoid?\nIs there any redundant features, which do not reveal much about the core patterns in the dataset?\nA common approach to answer these questions is to draw scatterplots between each pair of features. However, since there are up to 13 variables, we need to look at 78 (13*12/2) graphs, which can be time-consuming.\nA simpler and better solution is a correlogram, which shows the correlation between each feature.\n\n\ndata <- cor(wine[,1:13])\ncorre <- melt(data)\nggplot(filter(corre, as.integer(Var1) < as.integer(Var2)), aes(Var1, Var2, fill = value, size = abs(value))) + \n  geom_point(shape = 21, stroke = 0) + \n  scale_x_discrete(position = \"top\", name = NULL, expand = c(0, 0.8)) +\n  scale_y_discrete(name = NULL, expand = c(0, 0.8)) +\n  scale_size_area(max_size = 12, limits = c(0, 1), guide = \"none\")+\n  scale_fill_continuous_divergingx(\n    palette = \"PuOr\", rev = FALSE,\n    limits = c(-1, 1),\n    breaks = c(-1, 0, 1),\n    labels = c(\"–1\", \"0\", \"1\"),\n    name = \"correlation\",\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      label.position = \"bottom\",\n      title.position = \"top\",\n      barwidth = grid::unit(140, \"pt\"),\n      barheight = grid::unit(17.5, \"pt\"),\n      ticks.linewidth = 1\n    )\n  ) +\n  coord_fixed() +\n  labs(\n    caption = \"Source: Forina, M. et al, PARVUS\",\n    title = \"Correlation in wine components \",\n    subtitle = \"The size of circles denotes the magnitude of each correlation\") +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 8),\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = grid::unit(3, \"pt\"),\n    legend.position = c(.97, .0),\n    legend.justification = c(1, 0),\n    legend.title.align = 0.5,\n    legend.text = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    legend.title = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    plot.title = element_text(family = \"Avenir Next\", size = 13, color = \"#353d42\", face = \"bold\"),\n    plot.subtitle = element_text(family = \"Avenir Next\", size = 11, color = \"#353D42\", hjust = 0),\n    plot.caption = element_text(family = \"Avenir Next\", size = 10, color = \"#666666\", hjust = 0)\n  )\n\n\n\nBased on this graph, we may clearly see a strong positive correlation between Phenol and Flav or a strong negative correlation between Hue and Malic. However, this approach hides the underlying data points, which would lead to some incorrect conclusions. PCA is a better option to find patterns while still showing the raw data.\nWhat is PCA?\nPrincipal component analysis or PCA is a popular technique for dimensionality reduction and finding patterns in exploratory analysis.\nPCA can be used to deal with the so-called “the curse of dimensionality”, which refers to some problems with high-dimensional datasets. These include reduced estimation accuracy, higher computational expenses to handle data, and hard interpretation.\nPCA is based on an assumption that most high-dimensional datasets contain many correlated variables that show overlapping information. Thus, we may reduce the set to a smaller number of important features without affecting much core information.\nLet’s take a dataset of multiple physical features of students for example. Variables such as overall weight, height, lengths of legs and arms, circumferences of chest and hips, etc are typically correlated with each other. We may reduce them into a single feature called overall size. People with larger overall size tend to have these features higher. Other important feature can be sex and age. There are noticeable differences between the measurements of male and female or children and adults. So, from the original set of 50 or even 100 variables, we may reduce to only 3 key features that can still reveal important patterns in the data.\nHow PCA Works\nThe first step in PCA is to standardize the dataset to unit variance (var = 1). This ensures that variables measured in different units contribute equally to the analysis. Otherwise, features with larger ranges may dominate those with smaller ranges.\nFor instance, the alcohol variable with a range from 0-500 mg may dominate over a phenol variable with a range 0-10 g.\nThe formula for standardization: \\(\\frac{value \\:-\\: mean}{standard \\: deviation}\\)\nHere’s what the original dataset looks like after standardization.\n\n\nscaled_wine <- data.frame(scale(wine[,1:13]))\nrmarkdown::paged_table(head(scaled_wine))\n\n\n\n\nThe next step is to find the principle components, which are linear combinations of the original variables. We combine in a way that most of the variance or information of the original variables is compressed in the first principal component (PC1). Similarly, PC2 should account for the second largest amount of variation in the data.\nWe can perform PCA in R with the pcrcomp() function.\n\n\npca = prcomp(wine[,1:13], scale = TRUE)\nrmarkdown::paged_table(data.frame(pca$rotation))\n\n\n\n\nThe results show the loadings, which define the direction of the principal components. For example, PC1 and PC2 can be constructed as the following form:\n\\(PC1 = -0.144*Alcohol + 0.245*Malic + .... - 0.376*OD - 0.287*Proline\\)\n\\(PC2 = 0.484*Alcohol + 0.225*Malic + .... - 0.164*OD + 0.365*Proline\\)\n\n\nvar_explained <- data.frame(PC = paste0(\"PC\",1:13),\n                            var_explained = (pca$sdev)^2/sum((pca$sdev)^2)) \nvar_explained$PC <- factor(var_explained$PC, levels = c(\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\",\"PC11\",\"PC12\",\"PC13\"))\nscree_plot <- ggplot(var_explained, aes(x = PC, y = var_explained, group = 1)) +\n  geom_line(size = 0.2) +\n  geom_col(width = 0.5, fill = \"#56B4E9\") + \n  geom_point(size = 1)+\n  scale_y_continuous(expand = c(0,0), name = \"explained variance\") +\n  scale_x_discrete(expand = c(0,0), name = \"\")+\n  coord_cartesian(clip = \"off\") +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.5),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = \"Avenir Next\", color = \"#353d42\", size = 11),\n    axis.title = element_text(family = \"Avenir Next\", color = \"#353d42\", size = 11)\n  )\nggplotly(scree_plot)\n\n\n\nThe scree plot shows that PC1 and PC2 explains 36.2% and 19.2% of variation in the data. With the first 4 PCs, we can explain up to 73.5% of the information from the provided dataset with 13 variables.\nThe overall PCA process can be summarized by the following graph.\n\n\n\nInterpret PCA Results\nThere are two things that we are interested in the results of PCA:\nThe composition of principal components\nThe locations of each data point in the principal components space\nFirstly, we take a look at the composition of PC1 and PC2\n\n\npca <- prcomp(wine[,1:13], scale = TRUE)\nggbiplot(pca, alpha = 0) +\n  scale_x_continuous(name = \"PC1\", expand = c(0, 0.1), limits = c(-2, 1.5)) +\n  scale_y_continuous(name = \"PC2\", expand = c(0, 0.1), limits = c(-1, 1.5)) +\n  scale_color_manual(values = c(\"#0072B2\")) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = 0.1, color = \"#cbcbcb\" ),\n    axis.text = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    axis.title = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    axis.ticks = element_blank(),\n    legend.position = \"top\"\n  )\n\n\n\nThe arrows represent how original scaled variables contribute to the PCs. They have varying lengths because there are also other PCs. Shorter arrows indicate that they may contribute more to other higher-order PCs that are not shown in this graph.\nWe can see that PC1 is mainly contributed by non-flavonoid, alash, proa, flavonoid, and flavonoid. PC2 is mainly contributed by ash, mg, color, and alcohol. Other features contribute in varying amounts to both PC1 and PC2. Some conclusions from this graph:\nSamples with high levels of malic acid also tend to have high levels of alcalinity of ash and nonflavonoid phenols\nSamples with high levels of flavonoids also tend to have high levels of proanthocyanins and phenols\nSample with high levels of malic acid, ash, and nonflavonoid phenols tend to have low levels of flavonoids, proanthocyanins, phenols, and OCD\nSamples with high levels of alcohol also tend to have high levels of ash and color intensity\nSecondly, we examine the location of original data points in the 2D space of PC1 and PC2\n\n\nggbiplot(pca, group = wine.class, ellipse = TRUE) +\n  scale_x_continuous(name = \"PC1\", expand = c(0, 0.1)) +\n  scale_y_continuous(name = \"PC2\", expand = c(0, 0.1)) +\n  scale_color_brewer(name = \"None\", palette = \"Set1\") +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major = element_line(size = 0.1, color = \"#cbcbcb\"),\n    axis.text = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    axis.title = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    axis.ticks = element_blank(),\n    legend.position = \"right\",\n    legend.text = element_text(family = \"Avenir Next\", color = \"#353D42\", size = 11),\n    legend.title = element_blank(),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    legend.key = element_blank()\n  )\n\n\n\nWe can easily notice defined clusters of wine classes. Some conclusions:\nBarbera wines tend to have higher than average levels of malic acid, alcalinity of ash, and nonflavonoid phenols, as well as lower than average levels of flavonoids, proanthocyanins, and phenols\nBarolo wines tend to have lower than average levels of malic acid, alcalinity of ash, and nonflavonoid phenols, as well as higher than average levels of flavonoids, proanthocyanins, and phenols\nBoth barolo and barbera wines tend to have higher than average levels of color intensity, ash, alcohol, and magnesium\nGrignolion wines tend to have lower than average levels of color intensity, ash, alcohol, and magnesium\nThese results of PCA can be helpful for building a classification model to identify the class of a wine based on the contents.\n\n\n\n",
    "preview": "https://images.pexels.com/photos/1283219/pexels-photo-1283219.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2",
    "last_modified": "2022-12-13T20:59:30+01:00",
    "input_file": {}
  },
  {
    "path": "posts/berlin rent price prediction/",
    "title": "Predict rental prices in Berlin with simple and multiple linear regression",
    "description": {},
    "author": [
      {
        "name": "Vinh Hung le",
        "url": "vinhhle.github.io"
      }
    ],
    "date": "2020-01-23",
    "categories": [],
    "contents": "\n\nContents\nCleaning the Dataset\nExploratory Data Analysis\nModelling\nModel 1: Simple linear regression\nModel 2: Collinearity\nModel 3: Multiple linear regression\n\nPrediction\n\nBerlin is the capital and also the most populous city in Germany with more than 3.6 millions inhabitants (2019). People from all walks of life live, work, and study in this major metropolitan. Despite a developed real estate market, it is not simple to find an apartment that suits your budget and lifestyle.\nIn this project, we’ll work with a real estate dataset scraped by Corrie Bartelheimer from Immoscout24, the biggest real estate in Germany. It includes information about rental properties in all 16 states of Germany.\n\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(reshape2)\nlibrary(colorspace)\nlibrary(ggridges)\nfont = \"Avenir Next\"\ntext_color = \"#353D42\"\n\n# import data\ngermany_real_estate <- read.csv(\"/Users/huvi611/Downloads/immo_data.csv\") \n\n\nSome questions:\nWhat is the relationship between the base rent and other features, such as service, area, year of construction, and the availability of balcony, garden, kitchen, parking, etc?\nWhich feature has the most effect on the base rent?\nIs there an interaction effect between features?\nCan we predict future base rents based on the provided information?\nCleaning the Dataset\nLet’s start with cleaning up the mess. We first need to choose interested features and remove some unnecessary variables, NA values, and duplicate observations.\n\n\nberlin_real_estate <- germany_real_estate %>% \n  filter(regio1 == \"Berlin\") %>% \n  select(baserent = baseRent, service = serviceCharge, area = livingSpace, room = noRooms, year = yearConstructed,  parking = noParkSpaces, balcony, kitchen = hasKitchen, cellar, garden, interior = interiorQual, new = newlyConst, lift) %>% \n  drop_na() %>% \n  distinct()\n\nrmarkdown::paged_table(berlin_real_estate, options = list(cols.print = 5))\n\n\n\n\nThe dataset contains the following features about rental properties in Berlin:\n- baserent: base renting price (in euro)\n- service: extra costs, such as internet or electricity (in euro)\n- area: living space (in sqm)\n- room: number of rooms\n- year: construction year\n- parking: number of parking spaces\n- balcony: does the property has balcony?\n- kitchen: does the property has kitchen?\n- cellar: does the property has cellar?\n- garden: is there a garden?\n- interior: interior quality\n- new: a new property or not?\n- lift: is there a lift or not?\nNow we need to check whether outliers exist in the dataset. There are generally two types of outliers:\n- Outliers result from a mistake or error\n- Outliers represent real observations, but look very different to others\nThe simplest way to detect an outlier is to make a scatterplot. First, take a look at the following plot between baserent and service\n\n\nggplot (berlin_real_estate, aes(x = baserent, y = service)) + \n  geom_point(shape = 21, fill = \"#D55E00\", color = \"white\", size = 2) +\n  scale_x_continuous(name = \"base rent (euro)\") +\n  scale_y_continuous(name = \"service (euro)\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\n\n\n\nHere we can notice 5 outliers with the base rent price more than 10,000€ and 4 outliers with the service price more than 2,000€. These observations make it harder to figure out the overall pattern and may reduce the accuracy of the analysis as weell. We may discard them by setting thresholds: baserent < 10000 and service < 2,000.\nHere’s what we have after removing outliers.\n\n\nggplot(filter(berlin_real_estate, baserent < 10000 & service < 2000), aes(x = baserent, y = service)) + \n  geom_point(shape = 21, fill = \"#D55E00\", color = \"white\", size = 2) +\n  scale_x_continuous(name = \"base rent (euro)\") +\n  scale_y_continuous(name = \"service (euro)\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\n\n\n\nNow it is easier to notice that there seems to be a linear relationship between the base rent and service.\nWith the same procedure, we may filter observations with room < 25. Below graphs show before and after removing room outliers.\n\n\np1 <- ggplot(filter(berlin_real_estate, baserent < 10000), aes(x = baserent, y = room)) + \n  geom_point(shape = 21, fill = \"#D55E00\", color = \"white\", size = 2) +\n  scale_x_continuous(name = \"base rent (euro)\") +\n  scale_y_continuous(name = \"number of rooms\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\np2 <- ggplot(filter(berlin_real_estate, baserent < 10000 & room < 25), aes(x = baserent, y = room)) + \n  geom_point(shape = 21, fill = \"#D55E00\", color = \"white\", size = 2) +\n  scale_x_continuous(name = \"base rent (euro)\") +\n  scale_y_continuous(name = \"number of rooms\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\nplot_grid(\n  p1, NULL, p2,\n  nrow = 1, align = 'hv', rel_widths = c(1, .04, 1, .04, 1))\n\n\n\n\n\nberlin_adjusted <- berlin_real_estate %>% \n  filter(baserent < 10000,\n         service < 1500,\n         room < 25,\n         !interior %in% c(\"simple\"))\n\n\nExploratory Data Analysis\nNow let’s find out some interesting facts about the dataset. Start with the distribution of the base rent.\n\n\nggplot(berlin_adjusted, aes(x = baserent)) +\n  geom_histogram(fill = \"#56B4E9\", binwidth = 400, colour = \"white\") +\n  stat_bin(binwidth = 400, aes(y = ..count.., label=..count..), geom=\"text\", family = font, color = text_color, size = 3.5, vjust = -0.5) +\n  geom_vline(xintercept = mean(berlin_adjusted$baserent), size = 0.5, linetype = 2) +\n   geom_curve(aes(x = 2100, xend = 1700, y = 280, yend = 275),\n             color = text_color,\n             size = 0.2,\n             arrow = arrow(length = unit(0.01, \"npc\"))) +\n  ggplot2::annotate(\"text\", x = 2500, y = 270, \n           label = \"Averaged rent: 1586€\", \n           family = font,\n           color = text_color, \n           size = 3.5) +\n  scale_y_discrete(expand = c(0,16)) +\n  scale_x_continuous(expand = c(0,0), name = \"base rent (euro)\", breaks = seq(0,6000,500))+\n  theme(\n    panel.background = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 11),\n    axis.title = element_text(family = font, color = text_color, size = 11)  \n  )\n\n\n\nThe histogram shows most people have to pay between 500 and 3000 €/month for rental properties in Berlin range. The averaged base rent is around 1986€/month.\nBut that’s not the final expense. How much do they still have to pay for extra costs, such as heating, electricity, or interset? Let’s take a look at the distribution of service.\n\n\nggplot(berlin_adjusted, aes(x = service)) +\n  geom_histogram(fill = \"#56B4E9\", binwidth = 100, colour = \"white\") +\n  stat_bin(binwidth = 100, aes(y = ..count.., label=..count..), geom=\"text\", family = font, color = text_color, size = 3.5, vjust = -0.5) +\n  geom_vline(xintercept = mean(berlin_adjusted$service), size = 0.5, linetype = 2) +\n   geom_curve(aes(x = 400, xend = 300, y = 400, yend = 410),\n             color = text_color,\n             size = 0.2,\n             arrow = arrow(length = unit(0.01, \"npc\"))) +\n  ggplot2::annotate(\"text\", x = 450, y = 385, \n           label = \"Averaged service: 262€\", \n           family = font,\n           color = text_color, \n           size = 3.5) +\n  scale_y_discrete(expand = c(0,25)) +\n  scale_x_continuous(expand = c(0,0), name = \"service (euro)\", breaks = seq(0,2000,200))+\n  theme(\n    panel.background = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 11),\n    axis.title = element_text(family = font, color = text_color, size = 11)  \n  )\n\n\n\nThe service price mainly ranges between 100 and 400€/month with the average of around 260€/month.\nAnd how about the living space?\n\n\nggplot(berlin_adjusted, aes(x = area, y = ..count..)) +\n  geom_density_line(fill = \"#D55E00\", color = \"darkorange\", alpha = 0.5, bw = 4, kernel = \"gaussian\") +\n  scale_y_continuous(expand = c(0,0), name = \"scaled density\") +\n  scale_x_continuous(expand = c(0,0), name = \"area (sqm)\", breaks = seq(0, 400, 50)) +\n  geom_vline(xintercept = mean(berlin_adjusted$area), size = 0.5, linetype = 2) +\n   geom_curve(aes(x = 125, xend = 104, y = 13, yend = 14),\n             color = text_color,\n             size = 0.2,\n             arrow = arrow(length = unit(0.01, \"npc\"))) +\n  ggplot2::annotate(\"text\", x = 155, y = 12.5, \n           label = \"Averaged size: 100 sqm\", \n           family = font,\n           color = text_color, \n           size = 3.5) +\n  coord_cartesian(clip = \"off\") +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"#cbcbcb\", size = 0.2),\n    panel.grid.major.x = element_line(color = \"#cbcbcb\", size = 0.2),\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, color = text_color, size = 10),\n    axis.title = element_text(family = font, color = text_color, size = 11))\n\n\n\nThe density plot shows that the area of most rental rents range between 50 and 150 sqm.\nFor interior decoration, there are three options: normal, sophisticated, and luxuty. Let’s see which option is more prevalent in Berlin rental properties.\n\n\nberlin_interior <- berlin_adjusted %>% \n  group_by(interior) %>% \n  summarize(\n    count = n()/10)\n\nwaffle::waffle(\n  setNames(berlin_interior$count, berlin_interior$interior),\n  rows = 5,\n  xlab = \"1 square = 10 units\"\n)\n\n\n\nThe number of rental properties labelled with sophisticated dominates, even larger than the total of normal and luxury interior.\nModelling\nBefore starting with modelling, we need to transform some categorical into dummy variables for regression.\n\n\nberlin_final <- berlin_adjusted %>% \n  mutate(\n    balcony = as.numeric(balcony),\n    kitchen = as.numeric(kitchen),\n    cellar = as.numeric(cellar),\n    garden = as.numeric(garden),\n    new = as.numeric(new),\n    lift = as.numeric(lift),\n    interior = case_when(\n      interior == \"normal\" ~ 0,\n      interior == \"sophisticated\" ~ 1,\n      interior == \"luxury\" ~ 1\n    )\n  )\n\n\nNext, split the dataset into two parts: one for training and one for prediction.\n\n\nset.seed(1)\ntrain <- sample(nrow(berlin_final), 700)\nberlin_train <- berlin_final[train,]\nberlin_test <- berlin_final[-train,]\n\n\nLet’s start with the correlations between each pair of features.\n\n\n\nThe corrolegram shows very strong correlations between the base rent and other features, especially service, area, room, and interior.\nModel 1: Simple linear regression\nFirst, let’s take a look at the scatterplot between baserent and area.\n\n\nggplot(filter(berlin_train), aes(x = area, y = baserent)) + \n  geom_point(shape = 21, fill = \"#0072B2\", color = \"white\", size = 2) +\n  scale_x_continuous(name = \"area (sqm)\") +\n  scale_y_continuous(name = \"baserent (euro)\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\n\n\n\nIt shows that base rent and area seem to have a linear relationship. This means properties with more living space tend to have higher base rents. The linear regression for this relationship can be shown as:\n\\(baserent = a + b*area + ε\\), with ε is the error term, which includes other features that affect the base rent.\nWe can use the lm() function to find a and b.\n\n\nlm1 <- lm(baserent ~ area, data = berlin_train)\nsummary(lm1)\n\n\nCall:\nlm(formula = baserent ~ area, data = berlin_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2892.6  -313.7   -85.6   239.6  3163.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3640    49.4648  -0.169    0.866    \narea         15.8095     0.4452  35.514   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 539.7 on 698 degrees of freedom\nMultiple R-squared:  0.6437,    Adjusted R-squared:  0.6432 \nF-statistic:  1261 on 1 and 698 DF,  p-value: < 2.2e-16\n\nThe results show that both coefficients are statistically significant. The model now becomes:\n\\(baserent ≈ -8.3640 + 15.8*area\\)\nThis means an increase of 1 sqm in the area may increase the base rent up to 15.8€. However, the accuracy of the model is quite low simply because the base rent can be determined by many other factors.\nThe residual standard error (RSE) is the estimate of the standard deviation of ε. In this case, it means observed base rents deviate from the true value by around 539.7, on average. With the mean value of baserent around 1592€, the percentage error is 539.7/1592 ≈ 34%.\nThe R-squared is 0.6437, meaning that the model can explain around 64% of the variance.\nThe graph below show the regression line with the standard error band.\n\n\nggplot(filter(berlin_train), aes(x = area, y = baserent)) + \n  geom_point(shape = 21, fill = \"#0072B2\", color = \"white\", size = 2) +\n  stat_smooth(method = \"lm\", color = text_color, size = 0.5) +\n  scale_x_continuous(name = \"area (sqm)\") +\n  scale_y_continuous(name = \"baserent (euro)\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.text = element_text(family = font, size = 11, color = text_color),\n    axis.title = element_text(family = font, size = 11, color = text_color),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"#cbcbcb\", size = 0.3),\n    axis.title.x = element_text(margin = margin (t = 10)),\n    legend.position = \"none\")\n\n\n\nModel 2: Collinearity\nNow let’s add a another feature room into the model.\n\n\nlm2 <- lm(baserent ~ area + room, data = berlin_train)\nsummary(lm2)\n\n\nCall:\nlm(formula = baserent ~ area + room, data = berlin_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2965.15  -304.23   -83.49   242.55  3129.49 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16.9313    58.9697   0.287    0.774    \narea         16.2667     0.7311  22.249   <2e-16 ***\nroom        -23.6147    29.9516  -0.788    0.431    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 539.8 on 697 degrees of freedom\nMultiple R-squared:  0.6441,    Adjusted R-squared:  0.643 \nF-statistic: 630.6 on 2 and 697 DF,  p-value: < 2.2e-16\n\nThe results indicate that room is not a statistically significant variable. This can be explained by a phenomenon called collinearity. It occurs when two variables are highly linearly related. In this case, we may think that properties with more room tend to have larger living areas.\nModel 3: Multiple linear regression\nNow we regress the baserent on all features of the dataset.\n\n\nlm3 <- lm(baserent ~ ., data = berlin_train)\nsummary(lm3)\n\n\nCall:\nlm(formula = baserent ~ ., data = berlin_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2291.9  -256.4   -48.6   181.9  2516.9 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    3.5579  1033.9914   0.003  0.99726    \nservice        1.4856     0.1707   8.702  < 2e-16 ***\narea          10.6220     0.8768  12.115  < 2e-16 ***\nroom          33.3269    27.1021   1.230  0.21924    \nyear          -0.2360     0.5274  -0.447  0.65470    \nparking       -2.9201     3.4022  -0.858  0.39102    \nbalcony      -74.1553    61.6560  -1.203  0.22950    \nkitchen      116.6264    43.0636   2.708  0.00693 ** \ncellar        58.4286    43.8037   1.334  0.18269    \ngarden       -32.4150    39.9224  -0.812  0.41710    \ninterior     308.2498    53.7373   5.736 1.45e-08 ***\nnew          104.4353    46.7379   2.234  0.02577 *  \nlift         219.6438    44.1091   4.980 8.07e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 470.8 on 687 degrees of freedom\nMultiple R-squared:  0.7331,    Adjusted R-squared:  0.7285 \nF-statistic: 157.3 on 12 and 687 DF,  p-value: < 2.2e-16\n\nWith more features included, RSS is significantly reduced and R-squared is also much higher. Statically significant variables include service, area, kitchen, interior, new, and lift.\nPrediction\nOnce we have done with modelling, it’s time to use the model for predicting the base rent in the test dataset. From a list of features as inputs, we can calculated predicted prices.\nThe following dataframe shows the actual and predicted prices of the test dataset, using 3 models above:\n\n\npred.lm1 = predict(lm1, newdata = berlin_test)\npred.lm2 = predict(lm2, newdata = berlin_test)\npred.lm3 = predict(lm3, newdata = berlin_test)\nprediction <- data.frame(berlin_test$baserent, pred.lm1, pred.lm2, pred.lm3) %>% \n  rename(\n    actual_price = berlin_test.baserent,\n    predicted_model1 = pred.lm1,\n    predicted_model2 = pred.lm2,\n    predicted_model3 = pred.lm3\n  )\nrmarkdown::paged_table(prediction)\n\n\n\n\nTo measure the accuracy of this regression model, we may calculate the root-mean-square error (RMSE). This metric simply calculates the average error of all predictions.\n\n\nRMSE1 = sqrt(mean((prediction$actual_price - prediction$predicted_model1)^2))\nRMSE2 = sqrt(mean((prediction$actual_price - prediction$predicted_model2)^2))\nRMSE3 = sqrt(mean((prediction$actual_price - prediction$predicted_model3)^2))\ndata.frame(RMSE1, RMSE2, RMSE3)\n\n     RMSE1    RMSE2    RMSE3\n1 546.8717 544.9041 451.3863\n\nModel 3 has the smallest RMSE. So if we have to choose one, this should be a suitable option. This does not necessarily mean it is the best model for predicting future observations.\nTo increase the accuracy of prediction, we need to:\n- Increase the sample size by collecting more data on rental properties in Berlin\n- Include more relevant features linked to the base rent prices, such as the neighborhood, number of supermarkets or schools around, distance to the nearest bus or train station, etc.\n\n\n\n",
    "preview": "https://images.pexels.com/photos/129494/pexels-photo-129494.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2",
    "last_modified": "2022-12-13T20:58:46+01:00",
    "input_file": {}
  }
]
